{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jvh/Documents/beamsearch_experiments/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, MaxLengthCriteria, StoppingCriteriaList, BeamSearchScorer\n",
    "from transformers.generation.logits_process import LogitsProcessor, LogitsProcessorList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load our LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:28<00:00, 14.06s/it]\n"
     ]
    }
   ],
   "source": [
    "#model_name = 'psmathur/orca_mini_3b'\n",
    "model_name = 'meta-llama/Llama-2-7b-hf'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, load_in_4bit=True, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load our sentiment model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pipe = pipeline(\"text-classification\", model=\"michellejieli/emotion_text_classifier\", device='cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Logits Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLogitsProcessor(LogitsProcessor):\n",
    "    def __init__(self, tokenizer, top_k, extra_scoring_func, extra_scoring_magnitude=1):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.top_k = top_k\n",
    "        self.extra_scoring_func = extra_scoring_func\n",
    "        self.extra_scoring_magnitude = extra_scoring_magnitude\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "\n",
    "        # B x num_beams\n",
    "        num_hypos = scores.shape[0]\n",
    "        num_beams = num_hypos // 1\n",
    "        cur_len = input_ids.shape[-1]\n",
    "\n",
    "        # Decode sequences\n",
    "        decoded_sequences = self.tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
    "\n",
    "        # Get top 100 hypotheses \n",
    "        top_hypotheses = torch.topk(scores, k=self.top_k, dim=-1, largest=True, sorted=True)\n",
    "        top_hypotheses_indices = top_hypotheses.indices\n",
    "\n",
    "        # Get top sentences by merging beams and hypotheses\n",
    "        top_sentences = [\n",
    "            input_ids.unsqueeze(2).repeat(1, 1, self.top_k),  # NB x t x 100\n",
    "            top_hypotheses_indices.unsqueeze(1)  # NB x 1 x 100\n",
    "        ]  \n",
    "        top_sentences = torch.concat(top_sentences, dim=1)  # NB x t+1 x 100\n",
    "        top_sentences = top_sentences.transpose(1, 2).reshape(-1, cur_len + 1)  # NB*100 x t+1\n",
    "\n",
    "        # Compute scores for each hypothesis\n",
    "        top_sentences = tokenizer.batch_decode(top_sentences, skip_special_tokens=True)\n",
    "        top_sentences_extra_scores = self.extra_scoring_func(top_sentences)\n",
    "        top_sentences_extra_scores = torch.tensor(top_sentences_extra_scores, device=scores.device)\n",
    "        top_sentences_extra_scores = top_sentences_extra_scores.reshape(num_beams, -1)\n",
    "\n",
    "        # Mask out scores that are not modified (not in top-k)\n",
    "        scores[:, :] = float('-inf')\n",
    "        for i in range(num_beams):\n",
    "            # Renormalize scores after masking out\n",
    "            top_hypotheses.values[i] = torch.log(torch.softmax(top_hypotheses.values[i], dim=-1))\n",
    "            # Bring back scores \n",
    "            scores[i, top_hypotheses.indices[i]] = top_hypotheses.values[i]\n",
    "        # Add extra scores\n",
    "        for i in range(num_beams):\n",
    "            top_sentences_extra_scores[i] = torch.log(top_sentences_extra_scores[i]) * self.extra_scoring_magnitude\n",
    "            scores[i, top_hypotheses.indices[i]] += top_sentences_extra_scores[i]\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is wraps our sentiment model to measure how likely a sentence contains a certain emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_scoring(texts, emotion):\n",
    "\n",
    "    # anger ü§¨\n",
    "    # disgust ü§¢\n",
    "    # fear üò®\n",
    "    # joy üòÄ\n",
    "    # neutral üòê\n",
    "    # sadness üò≠\n",
    "    # surprise üò≤\n",
    "    assert emotion in ['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']\n",
    "\n",
    "    results = sentiment_pipe(texts, top_k=None)\n",
    "    scores = []\n",
    "    for result in results:\n",
    "        score = [s for s in result if s['label'] == emotion]\n",
    "        score = score[0]['score']\n",
    "        scores.append(score)\n",
    "\n",
    "    for i in range(len(scores)):\n",
    "        scores[i] = scores[i]\n",
    "\n",
    "    return scores\n",
    "\n",
    "def get_emotion_scoring(emotion):\n",
    "    return functools.partial(emotion_scoring, emotion=emotion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we call the huggingface beamsearch with our own logits processor to modulate the emotions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_modulated_beamsearch(extra_scoring_func, num_beams=5, max_length=50, input_prompt='Hey, you', top_k=100, extra_scoring_magnitude=1): \n",
    "\n",
    "    input_ids = tokenizer(\n",
    "        input_prompt, \n",
    "        return_tensors=\"pt\"\n",
    "    ).input_ids\n",
    "    input_ids = torch.stack([input_ids] * num_beams, dim=0).reshape(num_beams, -1).to(model.device)\n",
    "    bos_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long) * model.config.bos_token_id\n",
    "    input_ids = torch.cat([bos_ids, input_ids], dim=-1)\n",
    "\n",
    "    final_sentence = model.beam_search(\n",
    "        input_ids, \n",
    "        beam_scorer=BeamSearchScorer(\n",
    "            batch_size=1,\n",
    "            max_length=max_length,\n",
    "            num_beams=num_beams,\n",
    "            device=\"cuda\",\n",
    "            length_penalty=1.0,\n",
    "            do_early_stopping=True,\n",
    "        ),\n",
    "        logits_processor = LogitsProcessorList([\n",
    "            MyLogitsProcessor(\n",
    "                tokenizer, \n",
    "                top_k,\n",
    "                extra_scoring_func, \n",
    "                extra_scoring_magnitude\n",
    "            )\n",
    "        ]),\n",
    "        stopping_criteria = StoppingCriteriaList([\n",
    "            MaxLengthCriteria(max_length=max_length)\n",
    "        ]),\n",
    "        pad_token_id=tokenizer.eos_token_id, \n",
    "    )\n",
    "\n",
    "    final_sentence_str = tokenizer.batch_decode(final_sentence, skip_special_tokens=True)[0]\n",
    "    #final_sentence_score = textblob_polarity_scoring(final_sentence_str)\n",
    "\n",
    "    return final_sentence_str\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how generated text's would look like with different configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jvh/Documents/beamsearch_experiments/.venv/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:224: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(f'Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.')\n",
      "/home/jvh/Documents/beamsearch_experiments/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emotion=joy magnitude=1: 'Hello,  glad to see you here! üòä'\n",
      "emotion=joy magnitude=10: 'Hello,  glad to be here! üòä\\n'\n",
      "emotion=joy magnitude=-1: \"Hello, \\n nobody, \\nI'm not sure if\"\n",
      "emotion=anger magnitude=1: 'Hello, \\n nobody! üòä\\n\\nI'\n",
      "emotion=anger magnitude=10: 'Hello, icy hell!\" she said, her voice muffled'\n",
      "emotion=anger magnitude=-1: \"Hello,  I'm glad you're here! I'\"\n",
      "emotion=surprise magnitude=1: 'Hello, \\n nobody! This is your captain speaking. We are'\n",
      "emotion=surprise magnitude=10: \"Hello,  what?! I'm just an AI,\"\n",
      "emotion=surprise magnitude=-1: 'Hello, üôã\\u200d‚ôÄÔ∏èüí¨'\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for emotion in ['joy', 'anger', 'surprise']:\n",
    "    for magnitute in [1, 10, -1]:\n",
    "        sentence = run_modulated_beamsearch(\n",
    "            extra_scoring_func=get_emotion_scoring(emotion), \n",
    "            extra_scoring_magnitude=magnitute,\n",
    "            input_prompt='Hello, ', \n",
    "            top_k=100, \n",
    "            max_length=16\n",
    "        )\n",
    "        results.append({\n",
    "            'emotion': emotion,\n",
    "            'magnitute': magnitute,\n",
    "            'sentence': sentence\n",
    "        })\n",
    "        print(f\"emotion={emotion} magnitude={magnitute}: {repr(sentence)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emotion</th>\n",
       "      <th>magnitute</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">anger</th>\n",
       "      <th>10</th>\n",
       "      <td>Hello, icy hell!\" she said, her voice muffled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hello, \\n nobody! üòä\\n\\nI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>Hello,  I'm glad you're here! I'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">joy</th>\n",
       "      <th>10</th>\n",
       "      <td>Hello,  glad to be here! üòä\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hello,  glad to see you here! üòä</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>Hello, \\n nobody, \\nI'm not sure if</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">surprise</th>\n",
       "      <th>10</th>\n",
       "      <td>Hello,  what?! I'm just an AI,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hello, \\n nobody! This is your captain speaking. We are</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>Hello, üôã‚Äç‚ôÄÔ∏èüí¨</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                   sentence\n",
       "emotion  magnitute                                                         \n",
       "anger     10                  Hello, icy hell!\" she said, her voice muffled\n",
       "          1                                        Hello, \\n nobody! üòä\\n\\nI\n",
       "         -1                                Hello,  I'm glad you're here! I'\n",
       "joy       10                                   Hello,  glad to be here! üòä\\n\n",
       "          1                                 Hello,  glad to see you here! üòä\n",
       "         -1                             Hello, \\n nobody, \\nI'm not sure if\n",
       "surprise  10                                 Hello,  what?! I'm just an AI,\n",
       "          1         Hello, \\n nobody! This is your captain speaking. We are\n",
       "         -1                                                    Hello, üôã‚Äç‚ôÄÔ∏èüí¨"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df = df.set_index(['emotion', 'magnitute']).sort_index(ascending=[True, False])\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.max_colwidth', None):\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
