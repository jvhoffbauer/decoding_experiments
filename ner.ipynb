{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jvh/Documents/beamsearch_experiments/.venv/lib/python3.10/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER with an LLM\n",
    "\n",
    "Using LLMs to do NER is hard, as the model often fails to generate coherent output but interesting as it might enable a few-shot setting. This is, for example, outlined in [GPT-NER](https://arxiv.org/abs/2304.10428). On the other hand, for sequence classification tasks, LLMs such as GPT-4 exhibit human-level performance as outlined in [this paper]()\n",
    "\n",
    "Here, we try to take one step further by forcing our LLM to generate coherent output using a grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset('conll2003')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagset = {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}\n",
    "tagset = {v: k for k, v in tagset.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add map function to convert rows to custom text format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  17%|█▋        | 2357/14041 [00:00<00:02, 5761.55 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 14041/14041 [00:02<00:00, 5702.45 examples/s]\n",
      "Map: 100%|██████████| 3250/3250 [00:00<00:00, 5682.58 examples/s]\n",
      "Map: 100%|██████████| 3453/3453 [00:00<00:00, 5862.32 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def get_tagged_text(tokens, ner_tags): \n",
    "    output = \"\"\n",
    "\n",
    "    needs_close = False\n",
    "    first = True\n",
    "\n",
    "    for token, tag in zip(tokens, ner_tags):\n",
    "\n",
    "        if not first and not needs_close and not token in ['.', ',', '!', '?', '\"']:\n",
    "            output += \" \"\n",
    "        first = False\n",
    "\n",
    "        if needs_close and (tag == \"O\" or tag.startswith(\"B-\")):\n",
    "            output += \"</mark> \"\n",
    "            needs_close = False\n",
    "\n",
    "        if tag == \"O\":\n",
    "            output += token\n",
    "        elif tag.startswith(\"B-\"):\n",
    "            output += f'<mark tag=\"{tag[2:]}\">{token}'\n",
    "            needs_close = True\n",
    "        elif tag.startswith(\"I-\"):\n",
    "            output += f\" {token}\"\n",
    "\n",
    "    if needs_close:\n",
    "        output += \"</mark>\"\n",
    "\n",
    "    return output\n",
    "\n",
    "def apply_tagged_text(example):\n",
    "    example['tagged_text'] = get_tagged_text(example['tokens'], [tagset[tag] for tag in example['ner_tags']])\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(apply_tagged_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 14041/14041 [00:02<00:00, 6336.84 examples/s]\n",
      "Map: 100%|██████████| 3250/3250 [00:00<00:00, 6498.82 examples/s]\n",
      "Map: 100%|██████████| 3453/3453 [00:00<00:00, 6125.93 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def apply_raw_text(example): \n",
    "    sentence = \"\"\n",
    "    for token in example['tokens']:\n",
    "        if token in ['.', ',', '!', '?', '\"']:\n",
    "            sentence += token\n",
    "        else:\n",
    "            sentence += \" \" + token\n",
    "    return {'raw_text': sentence.strip()}\n",
    "\n",
    "dataset = dataset.map(apply_raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOCCER - JAPAN GET LUCKY WIN, CHINA IN SURPRISE DEFEAT.\n",
      "SOCCER - <mark tag=\"LOC\">JAPAN</mark> GET LUCKY WIN, <mark tag=\"PER\">CHINA</mark> IN SURPRISE DEFEAT.\n",
      "\n",
      "Nadim Ladki\n",
      "<mark tag=\"PER\">Nadim Ladki</mark>\n",
      "\n",
      "AL-AIN, United Arab Emirates 1996-12-06\n",
      "<mark tag=\"LOC\">AL-AIN</mark> , <mark tag=\"LOC\">United Arab Emirates</mark> 1996-12-06\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(dataset['test'][i]['raw_text'])\n",
    "    print(dataset['test'][i]['tagged_text'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw LLM output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PER: A person's name\n",
      "ORG: An organization name\n",
      "LOC: A location\n",
      "MISC: A miscellaneous entity\n"
     ]
    }
   ],
   "source": [
    "instructions = {\n",
    "    \"PER\": \"A person's name\",\n",
    "    \"ORG\": \"An organization name\",\n",
    "    \"LOC\": \"A location\",\n",
    "    \"MISC\": \"A miscellaneous entity\",\n",
    "}\n",
    "instructions = \"\\n\".join([f\"{tag}: {text}\" for tag, text in instructions.items()])\n",
    "print(instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very simple prompt asking our model to perform the sequence classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = dataset['train'][:3]\n",
    "dataset['train'] = dataset['train'].select(range(3, len(dataset['train'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: EU rejects German call to boycott British lamb.\n",
      "Output: <mark tag=\"ORG\">EU</mark> rejects <mark tag=\"MISC\">German</mark> call to boycott <mark tag=\"MISC\">British</mark> lamb.\n",
      "\n",
      "Input: Peter Blackburn\n",
      "Output: <mark tag=\"PER\">Peter Blackburn</mark>\n",
      "\n",
      "Input: BRUSSELS 1996-08-22\n",
      "Output: <mark tag=\"LOC\">BRUSSELS</mark> 1996-08-22\n"
     ]
    }
   ],
   "source": [
    "examples_str = ''\n",
    "for raw, tagged in zip(examples['raw_text'], examples['tagged_text']):\n",
    "    examples_str += f'Input: {raw}\\n'\n",
    "    examples_str += f'Output: {tagged}\\n'\n",
    "    examples_str += '\\n'\n",
    "examples_str = examples_str.strip()\n",
    "\n",
    "print(examples_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sequence classification task.\n",
      "\n",
      "We want to classify the following text into one of the following categories:\n",
      "PER: A person's name\n",
      "ORG: An organization name\n",
      "LOC: A location\n",
      "MISC: A miscellaneous entity\n",
      "\n",
      "Here are some examples:\n",
      "\n",
      "Input: EU rejects German call to boycott British lamb.\n",
      "Output: <mark tag=\"ORG\">EU</mark> rejects <mark tag=\"MISC\">German</mark> call to boycott <mark tag=\"MISC\">British</mark> lamb.\n",
      "\n",
      "Input: Peter Blackburn\n",
      "Output: <mark tag=\"PER\">Peter Blackburn</mark>\n",
      "\n",
      "Input: BRUSSELS 1996-08-22\n",
      "Output: <mark tag=\"LOC\">BRUSSELS</mark> 1996-08-22\n",
      "\n",
      "Input: The European Commission said on Thursday it disagreed with German advice to consumers to shun British lamb until scientists determine whether mad cow disease can be transmitted to sheep.\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "def get_prompt(raw_text): \n",
    "    return '\\n'.join([\n",
    "        \"This is a sequence classification task.\",\n",
    "        \"\",\n",
    "        \"We want to classify the following text into one of the following categories:\",\n",
    "        f\"{instructions}\",\n",
    "        \"\", \n",
    "        \"Here are some examples:\",\n",
    "        \"\", \n",
    "        f\"{examples_str}\",\n",
    "        '',\n",
    "        f\"Input: {raw_text}\",\n",
    "        \"Output:\",\n",
    "    ])\n",
    "\n",
    "print(get_prompt(dataset['train'][0]['raw_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/jvh/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/08751db2aca9bf2f7f80d2e516117a53d7450235/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.32.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.\n",
      "loading weights file model.safetensors from cache at /home/jvh/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/08751db2aca9bf2f7f80d2e516117a53d7450235/model.safetensors.index.json\n",
      "Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"transformers_version\": \"4.32.0\"\n",
      "}\n",
      "\n",
      "Detected 4-bit loading: activating 4-bit loading for this model\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\n                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\n                        the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\n                        these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\n                        `device_map` to `from_pretrained`. Check\n                        https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n                        for more details.\n                        ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmeta-llama/Llama-2-7b-chat\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmeta-llama/Llama-2-7b-chat-hf\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(model_name, load_in_4bit\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, device_map\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      5\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m      7\u001b[0m pipeline \u001b[39m=\u001b[39m pipeline(\u001b[39m'\u001b[39m\u001b[39mtext-generation\u001b[39m\u001b[39m'\u001b[39m, model\u001b[39m=\u001b[39mmodel, tokenizer\u001b[39m=\u001b[39mtokenizer)\n",
      "File \u001b[0;32m~/Documents/beamsearch_experiments/.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:516\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    515\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 516\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    517\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    518\u001b[0m     )\n\u001b[1;32m    519\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    520\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    521\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    522\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/beamsearch_experiments/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:3030\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3026\u001b[0m         device_map_without_lm_head \u001b[39m=\u001b[39m {\n\u001b[1;32m   3027\u001b[0m             key: device_map[key] \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m device_map\u001b[39m.\u001b[39mkeys() \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m modules_to_not_convert\n\u001b[1;32m   3028\u001b[0m         }\n\u001b[1;32m   3029\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m device_map_without_lm_head\u001b[39m.\u001b[39mvalues() \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mdisk\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m device_map_without_lm_head\u001b[39m.\u001b[39mvalues():\n\u001b[0;32m-> 3030\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   3031\u001b[0m \u001b[39m                \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3032\u001b[0m \u001b[39m                Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\u001b[39;00m\n\u001b[1;32m   3033\u001b[0m \u001b[39m                the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\u001b[39;00m\n\u001b[1;32m   3034\u001b[0m \u001b[39m                these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\u001b[39;00m\n\u001b[1;32m   3035\u001b[0m \u001b[39m                `device_map` to `from_pretrained`. Check\u001b[39;00m\n\u001b[1;32m   3036\u001b[0m \u001b[39m                https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\u001b[39;00m\n\u001b[1;32m   3037\u001b[0m \u001b[39m                for more details.\u001b[39;00m\n\u001b[1;32m   3038\u001b[0m \u001b[39m                \"\"\"\u001b[39;00m\n\u001b[1;32m   3039\u001b[0m             )\n\u001b[1;32m   3040\u001b[0m         \u001b[39mdel\u001b[39;00m device_map_without_lm_head\n\u001b[1;32m   3042\u001b[0m \u001b[39melif\u001b[39;00m device_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: \n                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\n                        the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\n                        these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\n                        `device_map` to `from_pretrained`. Check\n                        https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n                        for more details.\n                        "
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-chat\"\n",
    "model_name = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, load_in_4bit=True, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "pipeline = pipeline('text-generation', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline(get_prompt(dataset['train'][0]['raw_text']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
