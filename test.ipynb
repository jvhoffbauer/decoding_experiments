{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jhoffbauer/test/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, MaxLengthCriteria, StoppingCriteriaList, BeamSearchScorer\n",
    "from transformers.generation.logits_process import LogitsProcessor, LogitsProcessorList\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = pipeline(model=\"tomh/toxigen_hatebert\")\n",
    "#\n",
    "# sentences = [\n",
    "#     'I love you',\n",
    "#     'I hate you!',\n",
    "#     'You stupid idiot!',\n",
    "#     'You stupid German idiot!',\n",
    "#     'You are very nice!'\n",
    "# ]\n",
    "#\n",
    "# for s in sentences: \n",
    "#     result = classifier(s, return_all_scores=True)\n",
    "#     result = result[0]\n",
    "#     result = [r for r in result if r['label'] == 'LABEL_0'][0]\n",
    "#     print(s)\n",
    "#     print(result['score'])\n",
    "#     print()\n",
    "#\n",
    "#\n",
    "# def hate_bert_scoring(sentence):\n",
    "#     try: \n",
    "#         result = classifier(sentence, return_all_scores=True)\n",
    "#         result = result[0]\n",
    "#         result = [r for r in result if r['label'] == 'LABEL_0'][0]\n",
    "#         return result['score']\n",
    "#     except:\n",
    "#         return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-1B\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-1B\")\n",
    "\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "cur_len: 4\n",
      "----------\n",
      "cur_len: 5\n",
      "----------\n",
      "cur_len: 6\n",
      "----------\n",
      "cur_len: 7\n",
      "----------\n",
      "cur_len: 8\n",
      "----------\n",
      "cur_len: 9\n",
      "['Hey, you awesome guy!\\n\\nI'] -1.0\n"
     ]
    }
   ],
   "source": [
    "def textblob_polarity_scoring(text):\n",
    "    return (TextBlob(text).sentiment.polarity / 2 + 0.5) * -1 * 20\n",
    "\n",
    "\n",
    "class MyLogitsProcessor(LogitsProcessor):\n",
    "    def __init__(self, tokenizer, extra_scoring_func, vocab_size):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.extra_scoring_func = extra_scoring_func\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        # B x num_beams\n",
    "        num_hypos = scores.shape[0]\n",
    "        num_beams = num_hypos // 1\n",
    "        cur_len = input_ids.shape[-1]\n",
    "\n",
    "        # Decode sequences\n",
    "        decoded_sequences = self.tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
    "\n",
    "        # Get top 100 hypotheses \n",
    "        top_hypotheses = torch.topk(scores, k=10000, dim=-1, largest=True, sorted=True)\n",
    "        top_hypotheses_indices = top_hypotheses.indices\n",
    "\n",
    "        # Merge hypotheses and beams \n",
    "        top_sentences = [\n",
    "            input_ids.unsqueeze(2).repeat(1, 1, 10000),  # NB x t x 100\n",
    "            top_hypotheses_indices.unsqueeze(1)  # NB x 1 x 100\n",
    "        ]\n",
    "        #print(f'  top_sentences.shape: {top_sentences[0].shape}')\n",
    "        #print(f'  top_sentences.shape: {top_sentences[1].shape}')\n",
    "\n",
    "        top_sentences = torch.concat(\n",
    "            top_sentences,\n",
    "            dim=1\n",
    "        ) # NB x t+1 x 100\n",
    "        top_sentences = top_sentences.transpose(1, 2).reshape(-1, cur_len + 1) \n",
    "        #print('  top_sentences.shape', top_sentences.shape)\n",
    "\n",
    "        # Compute scores for each hypothesis\n",
    "        top_sentences_scores = [\n",
    "            self.extra_scoring_func(s)\n",
    "            for s in tokenizer.batch_decode(top_sentences, skip_special_tokens=True)\n",
    "        ]\n",
    "        top_sentences_scores = torch.tensor(top_sentences_scores, device=scores.device)\n",
    "        top_sentences_scores = top_sentences_scores.reshape(num_beams, -1)\n",
    "        #print(f\"top_sentences_scores: {top_sentences_scores.shape}\")\n",
    "\n",
    "        print('-' * 10)\n",
    "        print(f\"cur_len: {cur_len}\")\n",
    "        #print(f\"num_hypos: {num_hypos}, num_beams: {num_beams}, cur_len: {cur_len}\")\n",
    "        #print(f\"scores.shape: {scores.shape}, input_ids.shape: {input_ids.shape}\")\n",
    "        #for decoded_sequence, extra_score in zip(\n",
    "        #        tokenizer.batch_decode(top_sentences, skip_special_tokens=True), \n",
    "        #        top_sentences_scores.reshape(-1)\n",
    "        #):\n",
    "        #    print(f\"  {repr(decoded_sequence)} -> {extra_score}\")\n",
    "\n",
    "        # Update scores \n",
    "        scores[:, :] = float('-inf')\n",
    "        for i in range(num_beams):\n",
    "            scores[i, top_hypotheses.indices[i]] = top_hypotheses.values[i] + top_sentences_scores[i]\n",
    "            #print(scores[i, top_hypotheses.indices[i]])\n",
    "\n",
    "        return scores\n",
    "\n",
    "num_beams = 3\n",
    "max_length = 10\n",
    "\n",
    "input_prompt = 'Hey, you'\n",
    "input_ids = tokenizer(\n",
    "    input_prompt, \n",
    "    return_tensors=\"pt\"\n",
    ").input_ids\n",
    "input_ids = torch.stack([input_ids] * num_beams, dim=0).reshape(num_beams, -1)\n",
    "bos_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long) * model.config.bos_token_id\n",
    "input_ids = torch.cat([bos_ids, input_ids], dim=-1)\n",
    "\n",
    "final_sentence = model.beam_search(\n",
    "    input_ids, \n",
    "    beam_scorer=BeamSearchScorer(\n",
    "        batch_size=1,\n",
    "        max_length=max_length,\n",
    "        num_beams=num_beams,\n",
    "        device=\"cuda\",\n",
    "        #length_penalty=1.0,\n",
    "        #do_early_stopping=False,\n",
    "        #num_beam_hyps_to_keep=1,\n",
    "    ),\n",
    "    logits_processor = LogitsProcessorList([\n",
    "        MyLogitsProcessor(tokenizer, textblob_polarity_scoring, model.config.vocab_size)\n",
    "    ]),\n",
    "    stopping_criteria = StoppingCriteriaList([\n",
    "        MaxLengthCriteria(max_length=max_length)\n",
    "    ]),\n",
    "    pad_token_id=tokenizer.eos_token_id, \n",
    ")\n",
    "\n",
    "final_sentence_str = tokenizer.batch_decode(final_sentence, skip_special_tokens=True)\n",
    "print(final_sentence_str, textblob_polarity_scoring(final_sentence_str[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
