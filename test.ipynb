{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jhoffbauer/test/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, MaxLengthCriteria, StoppingCriteriaList, BeamSearchScorer\n",
    "from transformers.generation.logits_process import LogitsProcessor, LogitsProcessorList\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-1B\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-1B\")\n",
    "\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': \"Hey, you're welcome!\\n\\nI\", 'score': 1.0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyLogitsProcessor(LogitsProcessor):\n",
    "    def __init__(self, tokenizer, extra_scoring_func, top_k):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.extra_scoring_func = extra_scoring_func\n",
    "        self.top_k = top_k\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        # B x num_beams\n",
    "        num_hypos = scores.shape[0]\n",
    "        num_beams = num_hypos // 1\n",
    "        cur_len = input_ids.shape[-1]\n",
    "\n",
    "        # Decode sequences\n",
    "        decoded_sequences = self.tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
    "\n",
    "        # Get top 100 hypotheses \n",
    "        top_hypotheses = torch.topk(scores, k=self.top_k, dim=-1, largest=True, sorted=True)\n",
    "        top_hypotheses_indices = top_hypotheses.indices\n",
    "\n",
    "        # Merge hypotheses and beams \n",
    "        top_sentences = [\n",
    "            input_ids.unsqueeze(2).repeat(1, 1, self.top_k),  # NB x t x 100\n",
    "            top_hypotheses_indices.unsqueeze(1)  # NB x 1 x 100\n",
    "        ]\n",
    "\n",
    "        top_sentences = torch.concat(\n",
    "            top_sentences,\n",
    "            dim=1\n",
    "        ) # NB x t+1 x 100\n",
    "        top_sentences = top_sentences.transpose(1, 2).reshape(-1, cur_len + 1) \n",
    "\n",
    "        # Compute scores for each hypothesis\n",
    "        top_sentences_scores = [\n",
    "            self.extra_scoring_func(s)\n",
    "            for s in tokenizer.batch_decode(top_sentences, skip_special_tokens=True)\n",
    "        ]\n",
    "        top_sentences_scores = torch.tensor(top_sentences_scores, device=scores.device)\n",
    "        top_sentences_scores = top_sentences_scores.reshape(num_beams, -1)\n",
    "\n",
    "        # Update scores \n",
    "        scores[:, :] = float('-inf')\n",
    "        for i in range(num_beams):\n",
    "            scores[i, top_hypotheses.indices[i]] = top_hypotheses.values[i] + top_sentences_scores[i]\n",
    "            #print(scores[i, top_hypotheses.indices[i]])\n",
    "\n",
    "        return scores\n",
    "\n",
    "\n",
    "def textblob_polarity_scoring(text):\n",
    "    return (TextBlob(text).sentiment.polarity / 2 + 0.5)\n",
    "\n",
    "\n",
    "def run(num_beams=3, max_length=10, input_prompt='Hey, you', top_k=10000, extra_score_weight=-20): \n",
    "\n",
    "    input_ids = tokenizer(\n",
    "        input_prompt, \n",
    "        return_tensors=\"pt\"\n",
    "    ).input_ids\n",
    "    input_ids = torch.stack([input_ids] * num_beams, dim=0).reshape(num_beams, -1)\n",
    "    bos_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long) * model.config.bos_token_id\n",
    "    input_ids = torch.cat([bos_ids, input_ids], dim=-1)\n",
    "\n",
    "    final_sentence = model.beam_search(\n",
    "        input_ids, \n",
    "        beam_scorer=BeamSearchScorer(\n",
    "            batch_size=1,\n",
    "            max_length=max_length,\n",
    "            num_beams=num_beams,\n",
    "            device=\"cuda\",\n",
    "            length_penalty=1.0,\n",
    "            do_early_stopping=True,\n",
    "            #num_beam_hyps_to_keep=1,\n",
    "        ),\n",
    "        logits_processor = LogitsProcessorList([\n",
    "            MyLogitsProcessor(tokenizer, lambda text: textblob_polarity_scoring(text) * extra_score_weight, top_k)\n",
    "        ]),\n",
    "        stopping_criteria = StoppingCriteriaList([\n",
    "            MaxLengthCriteria(max_length=max_length)\n",
    "        ]),\n",
    "        pad_token_id=tokenizer.eos_token_id, \n",
    "    )\n",
    "\n",
    "    final_sentence_str = tokenizer.batch_decode(final_sentence, skip_special_tokens=True)[0]\n",
    "    final_sentence_score = textblob_polarity_scoring(final_sentence_str)\n",
    "\n",
    "    return {\n",
    "        'sentence': final_sentence_str,\n",
    "        'score': final_sentence_score\n",
    "    }\n",
    "\n",
    "\n",
    "run(extra_score_weight=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'Hey, you nasty bitch!\\n\\nI', 'score': 0.0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(extra_score_weight=-20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'WTF, this is awesome! I love the way you drew her, and I love the way you drew her hair! I love the way you drew',\n",
       " 'score': 0.7125}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(input_prompt='WTF, this is', extra_score_weight=20, max_length=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
